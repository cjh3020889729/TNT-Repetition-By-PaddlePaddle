{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transform In Transformer 介绍\n",
    "\n",
    "**TNT(Transform In Transformer)** 是继ViT 和 DeiT之后的优异vision transformer(视觉transformer)。在视觉任务上有较好性能。\n",
    "\n",
    "整体上来说，TNT较以前的模型在transformer处理上有更多的细节上的提升。\n",
    "\n",
    "提升点:\n",
    "\n",
    "  1. **patch-level + pixel-level** 两级结合，即利用Attention特性对patch-level上图像的**全局特征**进行高质量提取，同时又利用pixel-level对全局下的**局部特征**进行进一步提取，保证了图片的较为完整空间关系。\n",
    "\n",
    "  2. **position encode** 进行位置编码，保证图片在split时空间结构被较好地保留下来，这是以前的视觉transformer不具有的。\n",
    "\n",
    "  3. pixel-level的embedding实现嵌入后还将继续嵌入到patch-level的embedding中。\n",
    "\n",
    "  4. embedding的结果会与position encode进行结合，保证图片特征提取过程中**完整的空间结构**。\n",
    "   \n",
    "严谨性：\n",
    "\t\n",
    "  1. 利用消融实验，对**head数，position encode，two-level**的必要性进行了实验证明。\n",
    "  \n",
    "  \n",
    "> 下面就TNT复现代码进行讲解，适当的补充TNT体系结构的说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 完整实现代码\n",
    "\n",
    "\t-- models\n",
    "\t\t\n",
    "        * tnt_layers.py:  TNT模型所涉及到的组件网络层实现 + 代码注释\n",
    "        \n",
    "        * tnt_model.py:  TNT模型实现、与基本small、big模型的配置 + 代码注释\n",
    "        \n",
    "        * tset.py:     提供TNT模型的简单使用方法\n",
    "        \n",
    "        \n",
    "> 下边是一个测试文件，其它信息可前往test.py中查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/models\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "%cd models/\r\n",
    "from test import create_tnt_by_basecfg\r\n",
    "\r\n",
    "# on_start_test: True, 表示进行基本测试\r\n",
    "model = create_tnt_by_basecfg(num_classes = 10, img_size = 224, in_chans = 3, choice_big=False, on_start_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、TNT 模型基本流程解析\n",
    "\n",
    "(图源:论文)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7bb8e1ce274841918d0bea9e29efc5b00ff343594f76455bbbed1227a32968ee)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "基本流程如图中所示，基本步骤总结如下：\n",
    "\n",
    "\n",
    "| 步骤 | 组件 | info |\n",
    "| -------- | -------- | -------- |\n",
    "| 第一步     | Unfold + Conv2D     |   将输入图片分割成指定的pixel大小和patch大小   |\n",
    "| 第二步     | TNT Block     | 堆叠的TNT块将patch与pixel作为输入进行处理     |\n",
    "| 第三步     | inner transformer     | 在TNT中处理pixel     |\n",
    "| 第四步     | outer transformer     | 在TNT中处理patch    |\n",
    "| 第五步     | TNT Blocks     | 反复第二到第四步，直到第L块运行结束    |\n",
    "| 第六步     | MLP head     | 将TNT Blocks输出中的class_token部分作为输入，从而得到分类任务的输出结果    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、TNT代码解析\n",
    "\n",
    "就上边所说的流程进行代码复现，为了描述完整性，按照以下顺序介绍：\n",
    "\n",
    "1. Attention部分\n",
    "\n",
    "2. MLP部分\n",
    "\n",
    "3. DropPath部分(添加的丢弃策略)\n",
    "\n",
    "4. TNT Block部分\n",
    "\n",
    "5. Pixel Embed部分\n",
    "\n",
    "以上顺序与表格中略有不同，增添部分为论文代码实现的一些策略。\n",
    "\n",
    "> 具体内容，将在代码中去介绍，每一个部分的代码前边，会有对该部分代码的主要解析，与参数说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 0. 基本的依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle                              # 提供数据操作方法\r\n",
    "from paddle import nn                      # 网络层API\r\n",
    "from paddle.nn import functional as F      # 常见方法\r\n",
    "\r\n",
    "import math\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Attention部分\n",
    "\n",
    "注意力部分，主要是通过对输入数据进行注意力编码，然后将注意力结果叠加到原输入数据上。\n",
    "\n",
    "Attention前后数据形状不发生改变，仅发生数据值变化。\n",
    "\n",
    "主要实现思路:\n",
    "\n",
    "1. 将输入进行指定维度的线性映射(Linear)\n",
    "\n",
    "2. 第一次映射，同时获取questions 和 keys的映射数据\n",
    "\n",
    "3. 然后分离两者，并将两者进行矩阵乘积，然后通过softmax，计算question在key上的注意力结果\n",
    "\n",
    "4. 第二次映射，获取values的映射数据\n",
    "\n",
    "5. question与key的注意力结果与value进行矩阵乘积，将注意力作用到value上\n",
    "\n",
    "6. 最后将value映射回输入大小，适当丢弃，最后输出\n",
    "\n",
    "\n",
    "> 下边是代码实现 + 代码注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Layer):\r\n",
    "    '''\r\n",
    "        Multi-Head Attention\r\n",
    "    '''\r\n",
    "    def __init__(self, in_dims, hidden_dims, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., logging=False):\r\n",
    "        super(Attention, self).__init__()\r\n",
    "        ''' Attention\r\n",
    "            params list:\r\n",
    "                in_dims:    输入维度大小\r\n",
    "                hidden_dim: 隐藏层维度大小\r\n",
    "                num_heads:  注意力头数量\r\n",
    "                qkv_bias:   是否对question、keys、values开启映射\r\n",
    "                attn_drop:  注意力丢弃率\r\n",
    "                proj_drop:  映射丢弃率\r\n",
    "                logging:    是否输出Attention的init参数日志\r\n",
    "        '''\r\n",
    "\r\n",
    "        # 确保输入层、隐藏层维度为偶数，且不为零，否则在头划分映射时会发生大小错误\r\n",
    "        assert in_dims % 2 == 0 and in_dims != 0, \\\r\n",
    "            'please make sure the input_dims(now: {0}) is an even number.(%2==0 and !=0)'.format(in_dims)\r\n",
    "        assert hidden_dims % 2 == 0 and hidden_dims != 0, \\\r\n",
    "            'please make sure the hidden_dims(now: {0}) is an even number.(%2==0 and !=0)'.format(hidden_dims)\r\n",
    "\r\n",
    "        self.in_dims     = in_dims                           # ATT输入大小\r\n",
    "        self.hidden_dims = hidden_dims                       # ATT隐藏层大小\r\n",
    "        self.num_heads   = num_heads                         # ATT的头数目\r\n",
    "        self.head_dims   = hidden_dims // num_heads          # 将ATT隐藏层大小按照头数平分，作为ATT-head的维度大小\r\n",
    "        self.scale       = self.head_dims ** -0.5            # 缩放比例按照头的唯独大小进行开(-0.5次幂)\r\n",
    "        self.qkv_bias    = qkv_bias\r\n",
    "        self.attn_drop   = attn_drop\r\n",
    "        self.proj_drop   = proj_drop\r\n",
    "\r\n",
    "        # 输出日志信息\r\n",
    "        if logging:\r\n",
    "            print('\\n—— Attention Init-Logging ——')\r\n",
    "            print('{0:20}'.format(list(dict(in_dims=self.in_dims).keys())[0]),         ': {0:12}'.format(self.in_dims))\r\n",
    "            print('{0:20}'.format(list(dict(hidden_dims=self.hidden_dims).keys())[0]), ': {0:12}'.format(self.hidden_dims))\r\n",
    "            print('{0:20}'.format(list(dict(num_heads=self.num_heads).keys())[0]),     ': {0:12}'.format(self.num_heads))\r\n",
    "            print('{0:20}'.format(list(dict(head_dims=self.head_dims).keys())[0]),     ': {0:12}'.format(self.head_dims))\r\n",
    "            print('{0:20}'.format(list(dict(scale=self.scale).keys())[0]),             ': {0:12}'.format(self.scale))\r\n",
    "            print('{0:20}'.format(list(dict(qkv_bias=self.qkv_bias).keys())[0]),       ': {0:12}'.format(self.qkv_bias))\r\n",
    "            print('{0:20}'.format(list(dict(attn_drop=self.attn_drop).keys())[0]),     ': {0:12}'.format(self.attn_drop))\r\n",
    "            print('{0:20}'.format(list(dict(proj_drop=self.proj_drop).keys())[0]),     ': {0:12}'.format(self.proj_drop))\r\n",
    "\r\n",
    "        '''\r\n",
    "            questions + keys  |  values\r\n",
    "                    project layers\r\n",
    "        '''\r\n",
    "        # questions + keys 的映射层: *2 就是在一次操作下将两层一同映射\r\n",
    "        # qkv_bias：是否开启bias --> 开启默认全零初始化\r\n",
    "        self.qk = nn.Linear(self.in_dims, self.hidden_dims*2, bias_attr=qkv_bias)\r\n",
    "        # values 的映射层\r\n",
    "        # qkv_bias：是否开启bias --> 开启默认全零初始化\r\n",
    "        self.v  = nn.Linear(self.in_dims, self.in_dims, bias_attr=qkv_bias)\r\n",
    "        # ATT的丢弃层\r\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\r\n",
    "\r\n",
    "        '''\r\n",
    "            注意力结果映射层\r\n",
    "        '''\r\n",
    "        self.proj = nn.Linear(self.in_dims, self.in_dims)\r\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\r\n",
    "    \r\n",
    "\r\n",
    "    @paddle.jit.to_static\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = inputs\r\n",
    "        B, N, C= x.shape           # B:batch_size, N:patch_number, C:input_channel\r\n",
    "\r\n",
    "        # print('\\n—— Attention Forward-Logging ——')\r\n",
    "        # print('{0:20}'.format(list(dict(B=B).keys())[0]),                      ': {0:12}'.format(B))\r\n",
    "        # print('{0:20}'.format(list(dict(N=N).keys())[0]),                      ': {0:12}'.format(N))\r\n",
    "        # print('{0:20}'.format(list(dict(C=C).keys())[0]),                      ': {0:12}'.format(C))\r\n",
    "\r\n",
    "        # 利用输入映射question 和 keys维度的特征\r\n",
    "        # print('input(x): ', x.numpy().shape)\r\n",
    "        qk = self.qk(x)          # 将输入映射到question + keys上\r\n",
    "        # print('qk_project: ', qk.numpy().shape)\r\n",
    "        qk = paddle.reshape(qk, shape=(B, N, 2, self.num_heads, self.head_dims))   # 将question + keys分离\r\n",
    "        # print('qk_reshape: ', qk.numpy().shape)\r\n",
    "        qk = paddle.transpose(qk, perm=[2, 0, 3, 1, 4])   # 重新排列question和keys的数据排布\r\n",
    "        # print('qk_transpose: ', qk.numpy().shape)\r\n",
    "        '''\r\n",
    "            ①上面实现的划分，正好对应: head_dims = hidden_dims // num_heads\r\n",
    "            ②排布更新为: 映射类别(question+keys)，batch_size, head_number, patch_number, head_dims\r\n",
    "        '''\r\n",
    "        q, k = qk[0], qk[1]          # 分离question 和 keys\r\n",
    "        # print('q: ', q.numpy().shape)\r\n",
    "        # print('k: ', k.numpy().shape)\r\n",
    "\r\n",
    "        # 利用输入映射 values 维度的特征\r\n",
    "        v = self.v(x).reshape(shape=(B, N, self.num_heads, -1)).transpose(perm=(0, 2, 1, 3))\r\n",
    "        # print('v: ', v.numpy().shape)\r\n",
    "\r\n",
    "        # 通过question 与 keys矩阵积，计算patch的注意力结果\r\n",
    "        attn = paddle.matmul(q, k.transpose(perm=(0, 1, 3, 2))) * self.scale\r\n",
    "        # print('attn_matrix*: ', attn.numpy().shape)\r\n",
    "        '''\r\n",
    "            k.transpose(perm=(0, 1, 3, 2)) : 最后两维发生转置 --> 用于矩阵乘法，实现注意力大小计算(question 对 keys)\r\n",
    "            * self.scale : 针对注意力头数进行一定的缩放，稳定值\r\n",
    "        '''\r\n",
    "        attn = F.softmax(attn, axis=-1)          # 通过softmax整体估算注意力 -- 对每一个patch上的hidden_dim进行注意力计算\r\n",
    "        # print('attn_softmax: ', attn.numpy().shape)\r\n",
    "        attn = self.attn_drop(attn)              # 丢弃部分注意力结果\r\n",
    "\r\n",
    "        # 将注意力结果与value进行矩阵乘法结合\r\n",
    "        x = paddle.matmul(attn, v).transpose(perm=(0, 2, 1, 3)).reshape(shape=(B, N, -1))\r\n",
    "        # print('x_matrix*: ', x.numpy().shape)\r\n",
    "        ''' \r\n",
    "            attn 与 v 矩阵乘: 实现注意力叠加\r\n",
    "            transpose(perm=(0, 2, 1, 3)): 将patch与head维度互换(转置) -- 保证reshape不发生错误合并\r\n",
    "            reshape(shape=(B, N, -1)): 转换回:batch_size, patch_num, out_dims形式 -- out_dims = num_head * head_dims\r\n",
    "        '''\r\n",
    "        x = self.proj(x)          # 将注意力叠加完成的结果进行再映射，将其映射回输入大小\r\n",
    "        # print('x_proj: ', x.numpy().shape)\r\n",
    "        x = self.proj_drop(x)     # 丢弃部分结果\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. MLP部分\n",
    "\n",
    "多层感知机部分是整个过程中最简单的部分，只需要两层线性层(Linear)即可\n",
    "\n",
    "主要实现思路:\n",
    "\n",
    "1. 确定输入、输出、隐藏层大小\n",
    "\n",
    "2. 构建一层输入层，实现输入大小到隐藏大小的映射\n",
    "\n",
    "3. 构建一层输出层，实现隐藏大小到输出大小的映射\n",
    "\n",
    "4. 构建前向时，输入层后先跟GELU激活函数，再跟丢弃层\n",
    "\n",
    "5. 然后经过输出层 + 丢弃层，得到MLP的输出\n",
    "\n",
    "> 下边是代码实现 + 代码注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Layer):\r\n",
    "    '''\r\n",
    "        两层fc的感知机\r\n",
    "    '''\r\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\r\n",
    "        super(MLP, self).__init__()\r\n",
    "        ''' MLP\r\n",
    "            params list:\r\n",
    "                in_features:      输入大小\r\n",
    "                hidden_features:  隐藏层大小\r\n",
    "                out_features:     输出大小\r\n",
    "                act_layer:        激活层\r\n",
    "                drop:             丢弃率\r\n",
    "        '''\r\n",
    "\r\n",
    "        # 如果前项为None，则返回后向作为赋值内容\r\n",
    "        out_features    = out_features    or in_features\r\n",
    "        hidden_features = hidden_features or in_features\r\n",
    "        \r\n",
    "        # 第一层输入\r\n",
    "        self.fc1  = nn.Linear(in_features,     hidden_features)\r\n",
    "        self.act  = act_layer()\r\n",
    "        # 第二层输出\r\n",
    "        self.fc2  = nn.Linear(hidden_features, out_features)\r\n",
    "        self.drop = nn.Dropout()\r\n",
    "    \r\n",
    "\r\n",
    "    @paddle.jit.to_static\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = inputs\r\n",
    "        \r\n",
    "        x = self.fc1(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.drop(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = self.drop(x)\r\n",
    "\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. DropPath部分\n",
    "\n",
    "路径丢弃，与普通的Dropout相比，路径丢弃使得丢弃的数据更多。\n",
    "\n",
    "所谓路径，就是沿着shape中的第一个维度进行一整个的丢弃(全部置为0)；\n",
    "\n",
    "而普通的丢弃，仅仅对全部参数进行随机的丢弃，而DropPath则是将实现从逐个元素跨越到了整个轴上。\n",
    "\n",
    "> drop_path: 考虑到大面积丢弃，所以对未丢弃的数据的值进行了一定的扩增\n",
    "\n",
    "主要实现思路:\n",
    "\n",
    "1. 首先在运行前判断是否进行丢弃 —— drop为0，不丢弃；在训练中，不丢弃\n",
    "\n",
    "2. 计算保持率\n",
    "\n",
    "3. 得到路径丢弃的随机丢弃分布 -- random_tensor\n",
    "\n",
    "4. 扩增数据，并实现丢弃\n",
    "\n",
    "\n",
    "> 下边是代码实现 + 代码注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DropPath(nn.Layer):\r\n",
    "    '''删除路径数据\r\n",
    "        延一个路径进行丢弃(沿数据第一个维度进行丢弃)\r\n",
    "        丢弃的对应path下，所有数据置为0\r\n",
    "    '''\r\n",
    "    def __init__(self, drop_prob=None):\r\n",
    "        super(DropPath, self).__init__()\r\n",
    "        ''' DropPath\r\n",
    "            params list:\r\n",
    "                drop_prob:      丢弃率\r\n",
    "        '''\r\n",
    "        self.drop_prob = drop_prob\r\n",
    "\r\n",
    "\r\n",
    "    @paddle.jit.to_static\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = inputs\r\n",
    "        return self.drop_path(x)  # self.training是否在训练模型下\r\n",
    "\r\n",
    "\r\n",
    "    def drop_path(self, x):\r\n",
    "        '''\r\n",
    "            具体的path丢弃操作: 改变对应的值，不改变数据形状\r\n",
    "        '''\r\n",
    "        if self.drop_prob == 0. or not self.training:\r\n",
    "            return x\r\n",
    "        keep_prob = paddle.to_tensor([1 - self.drop_prob])\r\n",
    "        # 作batch_size维度大小的shape结构--(batch_size, 1, 1, ...)\r\n",
    "        shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)                        # batch_size, 1*原ndim减去batch_size维度的大小\r\n",
    "        random_tensor = keep_prob + paddle.rand(shape=shape, dtype=x.dtype)  # 按照划分的shape创建一个[0, 1)均匀分布随机tensor\r\n",
    "        # 利用[0,1)均匀分布产生的值 + 保持率，就可以实现等比例的保留和丢弃\r\n",
    "        # 由于随机性，可以保证丢弃的随机性\r\n",
    "        # 由于值总是在[0,1)间，所以只要得到的值 + keep_prob大于一个阈值，就保留\r\n",
    "        # 但是因为值时均匀分布的，虽然每一个位置上值时随机取到的，但是确实均匀划分的，\r\n",
    "        # 因此这样相加后可以实现对应丢弃概率下的丢弃path，并非一定会执行丢弃\r\n",
    "        # print(keep_prob + paddle.rand(shape=[2,]))\r\n",
    "        # print(paddle.floor(keep_prob + paddle.rand(shape=[2,])))\r\n",
    "        random_tensor = paddle.floor(random_tensor)           # 将1作为阈值，从而floor向下取整筛选满足的数据\r\n",
    "        # 仅仅留下 0, 1\r\n",
    "        # print(random_tensor)\r\n",
    "\r\n",
    "        # print(x[0, 0, 0])\r\n",
    "        # print(keep_prob)\r\n",
    "        # print(paddle.divide(x, keep_prob)[0, 0, 0])\r\n",
    "\r\n",
    "        # print(random_tensor.shape)\r\n",
    "        # print('x: ', x.numpy())\r\n",
    "        output = paddle.divide(x, keep_prob) * random_tensor\r\n",
    "        # print('output: ', output.numpy())\r\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. TNT Block部分\n",
    "\n",
    "TNT Block是TNT模型中的特征提取结构。\n",
    "\n",
    "TNT Block由Tin 与 Tout组成，利用Tin 对 pixel进行特征提取；利用Tout 对 patch进行特征提取。\n",
    "\n",
    "具体流程：先Tin，然后再Tout\n",
    "\n",
    "主要实现思路:\n",
    "\n",
    "1. 先构建Tin，利用Attention + MLP + Linear实现\n",
    "\n",
    "2. 输入的pixel_embed嵌入数据先通过Attention提取，作为当前层的pixel_embed结果，\n",
    "\n",
    "3. 再通过MLP进行映射提取，又进行叠加，得到当前block最终的pixel数据\n",
    "\n",
    "4. 然后利用Linear 对 pixel进行映射，实现从pixel到patch的映射，并将其加到patch_embed嵌入数据中\n",
    "\n",
    "5. 再构建Tout, 只需Attention + MLP\n",
    "\n",
    "6. 将上边加过pixel_embed的patch_embed依次通过Attention和MLP，得到Block的输出\n",
    "\n",
    "\n",
    "> 下边是代码实现 + 代码注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TNT_Block(nn.Layer):\r\n",
    "    '''\r\n",
    "        实现inner transfromer 和 outer transformer, 从pixel-level 和 patch-level进行数据特征提取\r\n",
    "\r\n",
    "        特性：\r\n",
    "            输入输出前后，tensor不发生shape变化（中间过程存在部分映射有shape变化）\r\n",
    "    '''\r\n",
    "    def __init__(self, patch_embeb_dim, in_dim, num_pixel, out_num_heads=12, \r\n",
    "                 in_num_head=4, mlp_ratio=4.,qkv_bias=False, drop=0., \r\n",
    "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\r\n",
    "        '''TNT_Block\r\n",
    "            params list:\r\n",
    "                patch_embeb_dim     : patch的嵌入维度大小(也是实际输入数据的映射空间大小)\r\n",
    "                in_dim              : 单个patch的维度大小(不包含pixel-level维度)\r\n",
    "                num_pixel           : patch下的(in_dim维度)元素对应pixel的比例 1：num_pixel，即pixel个数（也是论文中指的patch2pixel分辨率）\r\n",
    "                out_num_heads       : 输出(outer attn)的注意力头\r\n",
    "                in_num_head         : 输入(inner attn)的注意力头\r\n",
    "                mlp_ratio           : outer transformer中感知机隐藏层的维度缩放率\r\n",
    "                qkv_bias            : question 、 keys 、values对应的(线性)映射层bias启用标记\r\n",
    "                drop                : MLP部分、proj部分的丢弃率\r\n",
    "                attn_drop           : attn部分的丢弃率\r\n",
    "                drop_path           : 路径丢弃的丢弃率\r\n",
    "                act_layer           : 激活层\r\n",
    "                norm_layer          : 归一化层\r\n",
    "        ''' \r\n",
    "        super(TNT_Block, self).__init__()\r\n",
    "\r\n",
    "        # Inner transformer\r\n",
    "        # 输入-注意力计算 -- pixel level\r\n",
    "        self.in_attn_norm = norm_layer(in_dim)        # 层归一化--attention的归一化层\r\n",
    "        self.in_attn = Attention(in_dims=in_dim, hidden_dims=in_dim,\r\n",
    "                                 num_heads=in_num_head, qkv_bias=qkv_bias,\r\n",
    "                                 attn_drop=attn_drop, proj_drop=drop)         # attention输出，tensor的shape不变\r\n",
    "        # 输入-多层感知机进行维度映射\r\n",
    "        self.in_mlp_norm = norm_layer(in_dim)        # 层归一化--mlp的归一化层\r\n",
    "        self.in_mlp = MLP(in_features=in_dim, hidden_features=int(in_dim*4),\r\n",
    "                          out_features=in_dim, act_layer=act_layer, drop=drop) # mlp输出，tensor的shape不变\r\n",
    "        # 输入-线性映射输出\r\n",
    "        self.in_proj_norm = norm_layer(in_dim)        # 层归一化--proj的归一化层\r\n",
    "        self.in_proj = nn.Linear(in_dim * num_pixel, patch_embeb_dim, bias_attr=True)           # proj输出，tensor的shape发生改变\r\n",
    "\r\n",
    "\r\n",
    "        # outer transformer\r\n",
    "        # 输出-注意力计算 -- patch level\r\n",
    "        self.out_attn_norm = norm_layer(patch_embeb_dim)\r\n",
    "        self.out_attn = Attention(in_dims=patch_embeb_dim, hidden_dims=patch_embeb_dim,\r\n",
    "                                  num_heads=out_num_heads, qkv_bias=qkv_bias,\r\n",
    "                                  attn_drop=attn_drop, proj_drop=drop)\r\n",
    "        \r\n",
    "        self.out_mlp_norm = norm_layer(patch_embeb_dim)\r\n",
    "        self.out_mlp = MLP(in_features=patch_embeb_dim, hidden_features=int(patch_embeb_dim * mlp_ratio),\r\n",
    "                       out_features=patch_embeb_dim, act_layer=act_layer, drop=drop)\r\n",
    "\r\n",
    "        # 公用方法\r\n",
    "        # 路径丢弃\r\n",
    "        self.drop_path = DropPath(drop_prob=drop_path) if drop_path > 0. else self.Identity  # self.Identity()占位方法，不对数据做任何处理\r\n",
    "\r\n",
    "\r\n",
    "    @paddle.jit.to_static\r\n",
    "    def forward(self, pixel_embeb, patch_embeb):\r\n",
    "        '''\r\n",
    "            params list:\r\n",
    "                pixel_embeb: 上一个block输出的pixel-level out tensor\r\n",
    "                patch_embeb: 上一个block输出的patch-level out tensor\r\n",
    "        '''\r\n",
    "        # inner work\r\n",
    "        # 1. 注意力嵌入 added\r\n",
    "        pixel_embeb = pixel_embeb + self.drop_path(self.in_attn(self.in_attn_norm(pixel_embeb)))\r\n",
    "        # 2. mlp嵌入   added\r\n",
    "        pixel_embeb = pixel_embeb + self.drop_path(self.in_mlp(self.in_mlp_norm(pixel_embeb)))\r\n",
    "        # pixel嵌入的pathc叠加，在outer中完成\r\n",
    "\r\n",
    "        # outer work\r\n",
    "        B, N, C = patch_embeb.shape    # B:batch_size  N:Patch_Number  C:Feature_map_channel\r\n",
    "        # 线性映射pixel到patch维度，N-1 means；映射前后不包括class_token\r\n",
    "        # 映射是需要完整映射，不需要路径丢弃\r\n",
    "        pixel_embeb_proj2patch = self.in_proj(self.in_proj_norm(pixel_embeb).reshape(shape=(B, N-1, -1)))\r\n",
    "        # patch叠加上pixel的embeb数据，从patch1 --> patchn\r\n",
    "        # 不在这里操作class_token\r\n",
    "        patch_embeb[:, 1:] = patch_embeb[:, 1:] + pixel_embeb_proj2patch\r\n",
    "        # 1. 注意力嵌入 added\r\n",
    "        patch_embeb = patch_embeb + self.drop_path(self.out_attn(self.out_attn_norm(patch_embeb)))\r\n",
    "        # print(patch_embeb.shape)\r\n",
    "        # 2. mlp嵌入   added\r\n",
    "        patch_embeb = patch_embeb + self.drop_path(self.out_mlp(self.out_mlp_norm(patch_embeb)))\r\n",
    "        \r\n",
    "        return pixel_embeb, patch_embeb\r\n",
    "\r\n",
    "\r\n",
    "    def Identity(self, x):\r\n",
    "        '''\r\n",
    "            do nothing, only return input\r\n",
    "        '''\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Pixel Embed部分\n",
    "\n",
    "Pixel Embed的像素嵌入部分是整个TNT模型的入口，实现对图像进行合理提取，根据预置的patch大小等参数生成pixel的嵌入数据。\n",
    "\n",
    "【这里得到的pixel将可以在后边模型组网中实现从pixel2patch的直接映射——这就是TNT中提出的two-level extract(提取) features】\n",
    "\n",
    "主要实现思路:\n",
    "\n",
    "1. 根据给定的参数信息，进行一层卷积得到指定通道的特征图 -- 这是的通道数实际上就是单个pathc的维度（在不包含pixel级是的大小）【个人理解，如果有错误，可以在评论区指导一下，谢谢！】\n",
    "\n",
    "2. 根据stride，进行卷积提取——实际上卷积核大小与padding刚好可以保证卷积前后图像大小不变，但由于stride，导致特征图缩小了\n",
    "\n",
    "3. 再经过一个滑窗函数，进行滑窗展开，得到完整的pixel表示（pixel嵌入结果）\n",
    "\n",
    "4. 输出pixel嵌入结果\n",
    "\n",
    "> 下边是代码实现 + 代码注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Pixel_Embed(nn.Layer):\r\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, in_dim=48, stride=4):\r\n",
    "        super(Pixel_Embed, self).__init__()\r\n",
    "        '''Pixel_Embed: 像素嵌入--完成后才有patch嵌入\r\n",
    "            params_list:\r\n",
    "                img_size:   输入图片大小\r\n",
    "                patch_size: 当前一个patch的预置大小\r\n",
    "                in_chans:   输入的图像通道数\r\n",
    "                in_dim:     设定的输入维度 -- 即预定的patch的个数\r\n",
    "                stride:     分块时，使用卷积、滑窗的步长，决定着patch向下划分pixel时的分辨率(不是patch的分辨率)\r\n",
    "        '''\r\n",
    "        self.img_size = img_size\r\n",
    "        self.num_patches = (img_size // patch_size) ** 2\r\n",
    "        # 平方解释：img_size是宽时，//patch_size得到一行可划分多少个，而同样的列就有多少个\r\n",
    "        # 这里考虑完整划分patch的个数\r\n",
    "        self.in_dim = in_dim     # 每一个patch对应的分辨率 -- 即patch-level的分辨率\r\n",
    "        self.new_patch_size = math.ceil(patch_size / stride)   # 向上取整 -- 确定向下划分pixel的分辨率\r\n",
    "        self.stride = stride     # 卷积 + 滑窗的步长\r\n",
    "        \r\n",
    "        '''\r\n",
    "            两步实现图像到patch的映射，与patch到pixel的分割\r\n",
    "        '''\r\n",
    "        self.proj = nn.Conv2D(in_channels=in_chans, out_channels=self.in_dim,\r\n",
    "                              kernel_size=7, padding=3, stride=self.stride)\r\n",
    "        # 7 // 2 == 3, padding = 3, conv后会保持原图大小 -- 在stride=1时\r\n",
    "        self.unfold = F.unfold\r\n",
    "        # 对输入提取滑动块\r\n",
    "    \r\n",
    "\r\n",
    "    @paddle.jit.to_static\r\n",
    "    def forward(self, inputs, pixel_pos):\r\n",
    "        x = inputs\r\n",
    "        B, C, H, W = x.shape\r\n",
    "        # 验证是否与所需大小一致\r\n",
    "        assert H == self.img_size and W == self.img_size, \\\r\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size}).\"\r\n",
    "            \r\n",
    "        x = self.proj(x)\r\n",
    "        x = self.unfold(x, kernel_sizes=self.new_patch_size, strides=self.stride)   # 提取滑块，获得对应的滑块结果\r\n",
    "        # unfold将img转换为(B, Cout, Lout)\r\n",
    "        # Cout = Channel * kernel_sizes[0] * kernel_sizes[1]  , 即每一次滑窗在图片上得到参数个数\r\n",
    "        # Lout = hout * wout      —— 滑动block的个数\r\n",
    "        # hout，wout 类似卷积在图片对应h，w上的滑动次数\r\n",
    "        x = paddle.transpose(x, perm=[0, 2, 1])   # to be shape: (B, Lout, Cout)\r\n",
    "        x = paddle.reshape(x, shape=(B * self.num_patches, self.in_dim, self.new_patch_size, self.new_patch_size))\r\n",
    "        # 再分解为需要的编码形式: (Batch_size * patch_number, in_dim, new_patch_size, new_patch_size)\r\n",
    "        # Batch_size * patch_number：将每个batch得到的patch数乘以batch_size得到总的patch数量\r\n",
    "        # in_dim: 当前设定的输入维度大小\r\n",
    "        # new_patch_size: 由stride确定的 patch 的分辨率 -- 对应patch下feature map的w，h\r\n",
    "        x = x + pixel_pos           # 加上位置编码\r\n",
    "        x = paddle.reshape(x, shape=(B * self.num_patches, self.in_dim, -1))  # 拼接pixel-level的元素\r\n",
    "        x = paddle.transpose(x, perm=[0, 2, 1])\r\n",
    "        # 转换为(B * self.num_patches, patch_dim2pixel_size, self.in_dim)\r\n",
    "        # patch_dim2pixel_size: 即原in_dim下所有序列元素的拼接大小\r\n",
    "        # 原来是，in_dim对应dim下的pixels\r\n",
    "        # 现在是，每一个pixel对应in_dim的情况\r\n",
    "        \r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、TNT模型构建\n",
    "\n",
    "(图源:论文)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7bb8e1ce274841918d0bea9e29efc5b00ff343594f76455bbbed1227a32968ee)\n",
    "\n",
    "接下来就上面实现的组件代码进行模型构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "主要构建流程：\n",
    "\n",
    "1. 首先构建 Pixel Embed， 并搭建从pixel到patch的映射网络， 完成图上第一步和第二步的操作\n",
    "\n",
    "2. 创建class_token标记，为分类任务创建标记 -- 初始化采用随机截断正态分布\n",
    "\n",
    "3. 创建pixel 与 patch的position encoder，实现论文中的位置编码 -- 初始化采用随机截断正态分布\n",
    "\n",
    "4. 构建TNT Blocks，搭建特征提取主要网络 -- 完成图中第三和第四步\n",
    "\n",
    "5. 构建head，对TNT Blocks的输出的class_token进行指定任务的输出 -- 实现分类结果的输出 -- 完成后几步\n",
    "\n",
    "6. 预测结果并非限定在0-1.之间，实际使用预测，还需添加softmax函数进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TNT(nn.Layer):\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, \n",
    "                 num_classes=10, embed_dim=768, in_dim=48, depth=12,\n",
    "                 out_num_heads=12, in_num_head=4, mlp_ratio=4., qkv_bias=False, \n",
    "                 drop_rate=0., attn_drop_rate=0.,drop_path_rate=0., \n",
    "                 norm_layer=nn.LayerNorm, first_stride=4):\n",
    "        super(TNT, self).__init__()\n",
    "        '''TNT\n",
    "            params_list:\n",
    "                img_size:               输入图片大小（提前明确）\n",
    "                patch_size:             patch的大小（提前明确）\n",
    "                in_chans:               输入图片通道数\n",
    "                num_classes:            分类类别\n",
    "                embed_dim:              嵌入维度大小  -- 也是总的feature大小\n",
    "                in_dim:                 每个patch的维度大小 -- \n",
    "                                        但不是每个patch对应的实际全部元素，\n",
    "                                        全部元素还要加上in_dim[x]*每一个大小对应划分pixel的数量\n",
    "                depth:                  深度(block数量)\n",
    "                out_num_heads:          outer transformer的header数\n",
    "                in_num_head:            inner transformer的header数\n",
    "                mlp_ratio:              outer中mlp的隐藏层缩放比例\n",
    "                qkv_bias:               question、keys、values是否启用bias\n",
    "                drop_rate:              mlp、一般层（比如:映射输出、位置编码输出时）丢弃率\n",
    "                attn_drop_rate:         注意力丢弃率\n",
    "                drop_path_rate:         路径丢弃率\n",
    "                norm_layer:             归一化层\n",
    "                first_stride:           图片输入提取分块的步长--img --> patch\n",
    "        '''\n",
    "\n",
    "        self.num_classes = num_classes                      # 分类数\n",
    "        self.embed_dim = embed_dim                          # 嵌入维度 == 特征数\n",
    "        self.num_features = self.embed_dim                  # 嵌入维度 == 特征数\n",
    "\n",
    "        # 先完成pixel-level的嵌入\n",
    "        self.pixel_embeb = Pixel_Embed(img_size=img_size, patch_size=patch_size,\n",
    "                                       in_chans=in_chans, in_dim=in_dim, stride=first_stride)\n",
    "        self.num_patches = self.pixel_embeb.num_patches        # 当前pixel等效的实际的patch个数\n",
    "        self.new_patch_size = self.pixel_embeb.new_patch_size  # 当前等效的每一个patch对应的pixel的分辨率(w == h)\n",
    "        self.num_pixel = self.new_patch_size ** 2  # 当前每个patch实际划分的分辨率，w*h = w**2 得到patch2pixel的序列大小\n",
    "        \n",
    "        # 在进行patch-level嵌入\n",
    "        # 从pixel映射到patch上，要对每一个patch展开为pixel下的数据通过层归一化\n",
    "        self.first_proj_norm_start = norm_layer(self.num_pixel * in_dim)      # self.nwe_pixel * in_dim, 即每一个patch对应的全部元素\n",
    "        # 然后映射到指定嵌入维度上\n",
    "        self.first_proj = nn.Linear(self.num_pixel * in_dim, self.embed_dim)  # 将全部每一个patch对应的pixel都映射到指定嵌入维度大小的空间\n",
    "        # 在经过一次归一化，输出\n",
    "        self.first_proj_norm_end = norm_layer(self.embed_dim)\n",
    "\n",
    "        # 分类标记\n",
    "        # 截断正态分布来填充初始化cls_token、patch_pos、pixel_pos\n",
    "        self.cls_token = paddle.create_parameter(shape=(1, 1, self.embed_dim), dtype='float32', attr=nn.initializer.TruncatedNormal(std=0.02))\n",
    "        # 位置编码\n",
    "        # patch_position_encode: self.num_patches + 1对应实际patch数目+上边的分类标记\n",
    "        self.patch_pos = paddle.create_parameter(shape=(1, self.num_patches + 1, self.embed_dim), dtype='float32', attr=nn.initializer.TruncatedNormal(std=0.02))\n",
    "        # pixel_position_encode: in_dim对应每一个patch的大小, self.new_patch_size对应patch划分为pixel的分辨率\n",
    "        self.pixel_pos = paddle.create_parameter(shape=(1, in_dim, self.new_patch_size, self.new_patch_size), dtype='float32', attr=nn.initializer.TruncatedNormal(std=0.02))\n",
    "        # 位置编码的丢弃\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # 在TNT中使用了path_drop, 进行路径丢弃\n",
    "        # 为了丢弃更具随机性，提高鲁棒性，进行随机丢弃率的制作 -- 根据深度生成对应数量的丢弃率\n",
    "        drop_path_random_rates = [r.numpy().item() for r in paddle.linspace(0, drop_path_rate, depth)]\n",
    "        # 相同块，采用迭代生成\n",
    "        tnt_blocks = []\n",
    "        for i in range(depth):  # 根据深度添加TNT块\n",
    "            tnt_blocks.append(\n",
    "                TNT_Block(patch_embeb_dim=self.embed_dim, in_dim=in_dim, num_pixel=self.num_pixel,       # 嵌入大小， patch-level大小，patch2pixel大小\n",
    "                          out_num_heads=out_num_heads, in_num_head=in_num_head, mlp_ratio=mlp_ratio,     # outer transformer头数，inner transformer头数，感知机隐藏层缩放比\n",
    "                          qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,                   # transormer中bias启动情况，映射层的丢弃率， 注意力层的丢弃率\n",
    "                          drop_path=drop_path_random_rates[i], norm_layer=norm_layer)                    # 路径丢弃的丢弃率，归一化层\n",
    "            )\n",
    "        # 放入顺序层结构中\n",
    "        self.tnt_blocks = nn.Sequential(*tnt_blocks)                # 输入前后不发生shape变化\n",
    "        # tnt_blocks最后的输出还要经过一层归一化\n",
    "        self.tnt_block_end_norm = norm_layer(self.embed_dim)             # 沿用前边最初归一化层的嵌入大小进行归一化设置\n",
    "\n",
    "        # 输出任务结果 -- 这里是利用cls_token进行分类，embed_dim是整个模型的嵌入维度大小，也是cls_token的最后1维度的大小\n",
    "        self.head = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else TNT_Block.Identity()\n",
    "        \n",
    "        # 初始化网络参数\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        '''\n",
    "            完成整个网络的初始化工作\n",
    "        '''\n",
    "        for l in self.sublayers():\n",
    "            if isinstance(l, nn.Linear):\n",
    "                # 随即截断正态分布填充初始化\n",
    "                l.weight = paddle.create_parameter(shape = (l.weight.shape), dtype=l.weight.dtype,\n",
    "                                                   name=l.weight.name, attr=nn.initializer.TruncatedNormal(std=0.02))\n",
    "                # bias 默认开启就初始化为0  -- 不指定其它初始化方式时\n",
    "            elif isinstance(l, nn.LayerNorm):\n",
    "                # bias 默认开启就初始化为0  -- 不指定其它初始化方式时\n",
    "                # 常量1.0初始化\n",
    "                l.weight = paddle.create_parameter(shape = (l.weight.shape), dtype=l.weight.dtype,\n",
    "                                                   name=l.weight.name, attr=nn.initializer.Constant(value=1.0))\n",
    "\n",
    "    def get_classfier(self):\n",
    "        '''\n",
    "            用于获取分类任务头进行相应的任务预测、输出等\n",
    "            [在当前任务不是必要的]\n",
    "        '''\n",
    "        return self.head\n",
    "\n",
    "\n",
    "    def reset_classfier(self, num_classes):\n",
    "        '''\n",
    "            用于修改分类任务头的分类数目\n",
    "        '''\n",
    "        self.num_classes = num_classes  # 修改模型分类参数\n",
    "        self.head = nn.Linear(self.embeb_dim, self.num_classes) if self.num_classes > 0 else TNT_Block.Identity()\n",
    "        \n",
    "    \n",
    "    def upstream_forward(self, inputs):\n",
    "        '''\n",
    "            上有任务前向运算：TNT特征提取部分的网络运算\n",
    "        '''\n",
    "        x = inputs \n",
    "        B = x.shape[0]   # batch_size\n",
    "\n",
    "        # 1. 先进行pixel-level的嵌入\n",
    "        pixel_embeb = self.pixel_embeb(x, self.pixel_pos)\n",
    "\n",
    "        # 2. 再从pixel-level上升到patch-level的嵌入(即向上映射)\n",
    "        # 依次通过: pixel2patch的layer_norm, 然后进行映射，最后再通过一层layer_norm完成整个映射过程\n",
    "        # 其中输入的pixel_embeb要经过shape变换，将散布在不同in_dim下的参数进行拼接到对应patch下\n",
    "        patch_embeb = self.first_proj_norm_end(self.first_proj(self.first_proj_norm_start(pixel_embeb.reshape(shape=(B, self.num_patches, -1)))))\n",
    "        patch_embeb = paddle.concat([self.cls_token, patch_embeb], axis=1)    # 将分类任务标记拼接到patch的嵌入空间中\n",
    "        patch_embeb = patch_embeb + self.patch_pos   # 加上位置编码\n",
    "        patch_embeb = self.pos_drop(patch_embeb)     # 丢弃一部分编码结果\n",
    "\n",
    "        for tnt_block in self.tnt_blocks:            # 将前期处理好的嵌入信息，进行迭代，进行信息地进一步提取\n",
    "            pixel_embeb, patch_embeb = tnt_block(pixel_embeb, patch_embeb)\n",
    "        \n",
    "        patch_embeb = self.tnt_block_end_norm(patch_embeb)\n",
    "        \n",
    "        return patch_embeb[:, 0]        # 0号位置为cls_token对应的位置\n",
    "\n",
    "    @paddle.jit.to_static\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # 主体前向传播\n",
    "        x = self.upstream_forward(x)  # 返回cls_token，用于分类用\n",
    "        # 分类任务\n",
    "        x = self.head(x)  # 执行分类\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、简单总结\n",
    "\n",
    "复现TNT整体上来说没那么多复杂的设计，但是其Conv+滑窗、position encoder、two-level的设计思想却有一定的吸收难度。\n",
    "\n",
    "通过two-level，对全局特征和局部特征进行融合，改善以前的transformer在视觉上的不足。\n",
    "\n",
    "利用position encoder巩固图像的空间结构。\n",
    "\n",
    "参数调整建议:\n",
    "\t\n",
    "    1.in_num_head 尽量不动；\n",
    "    \n",
    "    2.改embeding_size可以按照64的倍数增加；\n",
    "    \n",
    "    3.改动size，dim等参数均要被2整除才可保证模型运行\n",
    "    \n",
    "    4.mlp_ratio 在过拟合时，可以适当缩小\n",
    "    \n",
    "> 具体的代码都在models中，test.py为测试代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 有问题欢迎评论区讨论\r\n",
    "\r\n",
    "> 姓名：蔡敬辉\r\n",
    "\r\n",
    "> 学历：大三（在读）\r\n",
    "\r\n",
    "> 爱好：喜欢参加一些大大小小的比赛，不限于计算机视觉——有共同爱好的小伙伴可以关注一下哦~后期会持续更新一些自制的竞赛baseline和一些竞赛经验分享\r\n",
    "\r\n",
    "> 主要方向：目标检测、图像分割与图像识别--在学习NLP, 正在捣鼓FPGA\r\n",
    "\r\n",
    "> 联系方式：qq:3020889729 微信:cjh3020889729\r\n",
    "\r\n",
    "> 学校：西南科技大学"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
